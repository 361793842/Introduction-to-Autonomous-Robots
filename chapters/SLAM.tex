\chapter{Simultaneous Localization and Mapping}\label{chap:slam}
Robots are able to keep track of their position and orientation, known as \emph{pose}, using a model of the noise arising in their drivetrain and their forward kinematics to propagate this error into a spatial probability density function (Section \ref{sec:errorprop}). If the robot sees uniquely identifiable landmarks with known locations, the variance of this distribution would shrink. This can be accomplished for discrete locations using Bayes' rule (Section \ref{sec:markovloc}) and for continuous distributions using the extended Kalman filter (Section \ref{sec:EKF}). The key insight here is that every observation will reduce the variance of the robot's position estimate. The Kalman filter performs an optimal fusion of two observations by weighting them inversely by their variance, i.e., unreliable observations count less than reliable ones. In the robot localization problem, one of the observations is typically comes from the robot's proprioceptive position observations (e.g.\ using wheel encoders or feed-forward control inputs) whereas the other observation comes from a landmark with known location on a map composed of ``landmarks.'' So far, we have assumed that these locations are known. This chapter will introduce:

\begin{itemize}
\item the concept of covariance (or, what all of the non-diagonal elements in the covariance matrix describe), and
\item how to estimate the robot's location and that of landmarks in the map at the same time (simultaneous localization and mapping, or SLAM)
\end{itemize}

\section{Introduction}
The SLAM problem has been a cornerstone problem of autonomous mobile robotics for a long time. It provides the foundation for a robot to be transported to an unknown location and being able to explore the area and build metrically accurate maps and pose estimates through onboard sensing alone. This could be useful for any field robot, whether terrestrial, extraterrestrial, under the ocean or in an unexplored built environment. This chapter will introduce one of the first comprehensive solutions to the problem to build understanding, even though it has since been superseded by computationally more efficient versions with a variety of algorithmic speed-ups and accuracy improvements. Let's begin by studying a series of special cases.

\subsection{Landmarks}
Since exteroceptive sensor measurements occur at a high frequency and generally must be processed in some way to reduce this data to algorithmically usable content, these measurements are generally distilled into features (\cref{chap:feature_extraction}). The features present in each sensor measurement, as explained in \cref{sec:line_recognition} for lines, are less numerous than the number of data points in each measurement, and may be matched across measurements repeatably despite slight viewpoint changes. Worth noting here is that not all features may be matched across a sequence of sensor measurements. Those features that may be matched reliably represent coherent structures in the world (e.g.\ a wall or edge) are geometrically pertinent. The structures are known as \emph{landmarks}; \index{Landmark} they are geometric objects in the real world that can be used to inform motion within the world.

\subsection{Special Case I: one landmark}
Consider an environment about which you know that it has only a single landmark, but the position of the landmark is unknown. %This might happen as a result of identifying this one matched feature over multiple measurements of the environment, or having an algorithm that culls features and yields only a single landmark.
 We assume that the robot is able to obtain the relative range and angle of this landmark, each with some variance. This landmark could be a tower, but also a graphical tag that the robot can uniquely identify. The position of this measurement of the landmark $m_i=[\alpha_i,r_i]$  in global coordinates is unknown, but can be calculated if an estimate of the robot's position $\boldsymbol{\hat{x}_k}$ is known.  The variance of $ m_i$'s components is now the variance of the robot's position plus the variance of the observation.

Now consider the robot moving toward the landmark and obtaining additional observations of it. Although the robot's position uncertainty is growing as it moves, it can now rely on the landmark $m_i$ to reduce the variance of its prior position (as long as the landmark is stationary). Repeated observations of the same landmark from different angles and distances might improve the quality of its estimation of the landmark's position, and hence, its own position. The robot therefore has a chance to keep its variance very close to that with which it initially observed the landmark and stored it into its map!

How does the arithmetic operation of this measurement fusion proceed? As you may have already guessed, this probabilistic updating can be accomplished through the use of the EKF framework from Section \ref{sec:EKF}. In that treatment, we assumed that landmarks have a deterministic location, but that the robot's sensing introduces a variance. This variance was propagated into the covariance matrix of the innovation ($ \boldsymbol{S}$). We can now simply add the variance of the estimate of the landmark's position to that of the robot's sensing process.

\subsection{Special Case II: two landmarks}
Consider now a map that has two landmarks. Visiting one after the other, the robot will be able to store both of them in its map, although the second landmark's location will be observed with higher variance due to the increasing positional variance with time. Although the observations of both landmarks are independent from each other, the relationship between their variances depend on the trajectory of the robot. The differences between these two variances are much lower if the robot observes them by moving in a straight line than when it performs a series of turns between them, since turns introduce greater variance in position.

As a thought experiment, consider this: a robot is driving for quite a long time and has accumulated a large variance in its position. It then observes the landmarks, one after the other, in a short period of time. The result of this would be that the probability density function over the distance between the two landmarks would have be narrowly distributed. This probability density function can be understood as the \emph{covariance} of the two random variables (each consisting of range and angle). In probability theory, the covariance is the measure of how much two variables are changing with respect to one another. Obviously, the covariance between the locations of two landmarks that are visited immediately after each other by a robot is much larger in magnitude than if those landmarks were to be observed far apart. This does not indicate that there is greater uncertainty in the landmarks' locations, rather that there is correlation between the variables. It should therefore be possible to use the covariance between landmarks to correct estimates of landmarks in retrospect. If the robot returns to the first landmark it has observed, it will be able to reduce the variance of its position estimate. As it knows that it has not traveled very far since it observed the last landmark, it can then correct this landmark's position estimate.

\section{The Covariance Matrix}
When estimating quantities with multiple variables, such as the position of a robot that consists of its $x$-position, its $y$-position and its orientation, matrix notation is a convenient way of writing down the relationships between them. For error propagation, we have written the variances of each input variable into the diagonal of a covariance matrix. For example, when using a differential wheel robot, uncertainty in position expressed by $ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$ were grounded in the uncertainty of its left and right wheel. We entered the variances of the left and right wheel into a $2 \times 2$ matrix and obtained a $3 \times 3$ matrix that had $ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$ on its diagonal. Here, we set all other entries of the matrix to zero and ignored entries in the resulting matrix that were not on its diagonal. The reason we could do this is because the uncertainties in the left and right wheels are independent random processes: there is no reason that the left wheel slips, just because the right wheel slips. Thus the covariance---the measure on how much two random variables are changing together---of these is zero. This is not the case for the robot's position: uncertainty in one wheel will affect all output random variables ($ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$) at the same time, which is expressed by their non-zero covariances. Therefore, there will be non-zero entries off the diagonal of the output covariance matrix.

In the context of SLAM, we will maintain the poses of all landmarks that the robot is aware of in a column vector. There variances will make up the diagonal of a large covariance matrix. As the robot visits consecutive landmarks there variances are correlated, leading to non-zero diagonal entries. 

\section{EKF SLAM}\label{sec:ekfslam}\label{sec:ekfslam}
The key idea in EKF SLAM is to extend the state vector from the robot's position (and potentially pose) to contain the position of all landmarks. Thus, the state:
\begin{equation}
\hat{\boldsymbol{x}}_{k'|k-1}=(x,y,\theta)^T,
\end{equation}
\noindent becomes
\begin{equation}
\hat{\boldsymbol{x}}_{k}=(x,y,\theta,\alpha_1,r_1,\ldots,\alpha_N,r_N)^T,
\end{equation}
\noindent assuming $ N$ landmarks, which is a $(3+2N)\times 1$ vector. The action update (or ``prediction update'') is identical to that as though the landmarks are already known; the robot simply updates its position using odometry and updates the variance of its position using error propagation. The covariance matrix is now a $(3+2N) \times (3+2N)$ matrix that initially holds the variances on position and those of each landmark on its diagonal.

What about the perception update? Here it is worth noting that only one landmark is observed at a time; even if they are observed at nearly identical times, the algorithm requires only observing one landmark first, and then the next. Thus, if the robot observes multiple landmarks at once, one needs to do multiple, consecutive perception updates. In practice this implies that only those values of the observation vector (a $(3+2N) \times 1$ vector) that correspond to the landmark that you observe will be nonzero. Similar considerations apply to the observation function and its Jacobian.

\subsection{Algorithm} \label{sec:ekfslam_alg}
We now introduce the algorithm for EKF SLAM, which is based on an iterative re-approximation scheme of the state vector and its corresponding covariance matrix. The state vector now includes the robot's position (potentially pose) and the position of all landmarks. The process proceeds as follows.

\subsubsection{Initialization}
To initialize the state vector, first set all of its entries to zeros (begin by assuming no landmarks in the environment):

\begin{equation}
    \boldsymbol{x}_0 = (0,0,0)^T,
\end{equation}

\noindent and set its covariance to a small number $\epsilon$:

\begin{equation}
    \boldsymbol{P}_0 = \begin{bmatrix} \epsilon & 0 & 0 \\ 0 & \epsilon & 0 \\ 0 & 0 & \epsilon \end{bmatrix}.
\end{equation}

This is due to the fact that one cannot know any quantity definitively, and also that otherwise the zero matrix would be uninvertible.

\subsubsection{Update}
If the robot is under motion and its sensors are providing information on landmarks of the map, then the state vector will be augmented in time while the pose of the robot will also be updated for each time-step. In EKF SLAM, both the sensor model and the process model may be non-linear, so we will need to calculate the Jacobian of these functions with respect to the state and covariance matrices.

This update is a two-step process: first the prediction update, followed by the perception update.

\paragraph{The prediction update.} If $f$ is the nonlinear transition model for the system and $\boldsymbol{u}$ are hypothetical control inputs, then the state prediction update is given as:

\begin{equation}
    \hat{\boldsymbol{x}}_{k'|k-1}= f(\hat{\boldsymbol{x}}_{k-1},\boldsymbol{u}_{k-1}).
\end{equation}

Meanwhile, the covariance prediction update is:

\begin{equation}
    \hat{\boldsymbol{P}}_{k'|k-1} = \boldsymbol{F}_{\hat{\boldsymbol{x}}_{k-1}} \boldsymbol{P}_{k-1} \boldsymbol{F}_{\hat{\boldsymbol{x}}_{k-1}}^T + \boldsymbol{N},
\end{equation}

\noindent where $\boldsymbol{F}_{\hat{\boldsymbol{x}}_{k-1}} = \frac{\partial f(\boldsymbol{x},\boldsymbol{u})}{\partial \boldsymbol{x}}|_{k-1}$ denotes the Jacobian matrix of the nonlinear transition model with respect to the state variable evaluated at $k-1$, and $\boldsymbol{N}$ is the covariance matrix of noise affecting the system's actuators (assumed to be additive in the state).

A remarkable result is that only the robot's state (and \emph{not} the landmark positions in the world frame) is dependent on $k$, so most of $\hat{\boldsymbol{P}}_{k'|k-1}$ will not be updated in this step.

\paragraph{The perception update.} We assume that the sensor observation function $h(\boldsymbol{x})$ may be nonlinear and is affected by additive noise with covariance $\boldsymbol{R}$. The actual noisy measurement coming from the sensors will be denoted as $\boldsymbol{y}_{k}$. The Jacobian of the observation function evaluated at the prior timestep is $\boldsymbol{H}_{\hat{\boldsymbol{x}}_{k'}} = \frac{\partial h(\boldsymbol{x})}{\partial \boldsymbol{x}}|_{k'}$. This update operates on the results from the prediction update in order to provide a ``fully fused'' state estimate. The state perception update, which results in the next state estimate and corresponding state covariance estimate, is:

\begin{equation}
    \hat{\boldsymbol{x}}_{k|k-1}= \hat{\boldsymbol{x}}_{k'|k-1} + \boldsymbol{K}_{k'}(\boldsymbol{y}_{k} - h(\hat{\boldsymbol{x}}_{k'|k-1})),
\end{equation}

\noindent and

\begin{equation}
    \hat{\boldsymbol{P}}_{k|k-1}= \hat{\boldsymbol{P}}_{k'|k-1} - \boldsymbol{K}_{k'} \boldsymbol{Z}_{k'} \boldsymbol{K}_{k'}^T,
\end{equation}

\noindent where $\boldsymbol{Z}_{k'}$ and $\boldsymbol{K}_{k'}$ are:

\begin{equation}
    \boldsymbol{Z}_{k'} = \boldsymbol{H}_{\hat{\boldsymbol{x}}_{k'}} \hat{\boldsymbol{P}}_{k'|k-1} \boldsymbol{H}_{\hat{\boldsymbol{x}}_{k'}}^T + \boldsymbol{R},
\end{equation}

\noindent and

\begin{equation}
    \boldsymbol{K}_{k'} = \hat{\boldsymbol{P}}_{k'|k-1} \boldsymbol{H}_{\hat{\boldsymbol{x}}_{k'}}^T \boldsymbol{Z}_{k'}
\end{equation}

\noindent respectively. Crucially this step requires the careful and error-free association of features with landmarks in a process known as data association \index{data association}. Data association can be accomplished through using description vectors over the features and matching if the similarity of the descriptors are above a certain threshold, or through a Hungarian algorithm for optimal assignment. This data association step can be avoided if the landmarks are uniquely labeled and do not rely on feature matching. It may also depend on the generation of new landmarks or the deletion of old ones to keep the state vector of bounded length.

Note that there are some significant computational speedups associated with the matrix inversions and multiplications in the preceeding update equations due to their sparsity, which we do not cover explicitly here. In total, the total complexity for the updates is $\mathcal{O}(kn^2)$, where $k$ is the number of landmarks and $n$ is the number of states. There are ways to in fact make this algorithm constant-time through a process known as \emph{marginalization} \cite{sibley2010sliding}, wherein some variables are effectively no longer re-estimated (e.g.\ old poses which are no longer affecting the current pose significantly).

\subsection{Multiple Sensors}
The use of sensors in robotics is riddled with multiple cost-benefit analyses. A vision sensor provides information on the structure of the environment and pose-to-pose information, e.g.\ through frame-to-frame alignment. However it succumbs in low-light or textureless environments, where frame alignment may no longer be feasible due to inadequate information content. A depth sensor can provide information on structure but can fail in many odometric tasks due to redundant geometry (e.g.\ in a building corridor). Finally, an IMU can provide short-term odometry estimates intrinsically, but without any environmental sensing, quickly drifts and diverges. There are constantly more sensors being brought into the mix of this cost-benefit analysis, but one can only add so many sensors to a platform due to size, weight, power, and cost constraints. The choice of sensors must be made based on the expected environment and design considerations.

Once a subselection of sensors has been determined, these sensors can be integrated into an EKF SLAM system through a simple augmentation. In this case, the Update step in Section \ref{sec:ekfslam_alg} may be augmented with an arbitrary number of sensors to be robust to failure modes in any one sensor. For instance, an IMU can provide orientation observations interleaved with a range sensor providing orientation and position observations based on landmarks. The overall algorithm for EKF SLAM is unchanged here, but merely introduces more update steps at different frequencies.

\section{Graph-based SLAM}
Usually, a robot obtains an initial estimate of where it is using some onboard sensors (odometry, optical flow, etc.), uses this estimate to localize landmarks (walls, corners, graphical patterns) in the environment, and finally refines its pose estimate by matching sensor information in consecutive fields using for example the ICP algorithm (Section \ref{sec:ICP}). As soon as a robot revisits the same landmark twice, it can update the estimate on its location. As consecutive observations are not independent, but rather closely correlated, the refined estimate can then be propagated along the robot's path. This is formalized in EKF-based SLAM. A more intuitive understanding is provided by a spring-mass analogy: each possible pose (mass) is constrained to its neighboring pose by a spring. The higher the uncertainty of the relative transformation between two poses (e.g., obtained using odometry), the weaker the spring. Every time a robot gains confidence on a relative pose, the spring is stiffened instead. Eventually, all poses will be pulled in place. This can be achieved by numerically reducing the overall error based on all available observations using gradiant descent and is known as \emph{Graph-based SLAM}\index{Graph-based SLAM}, see also \cite{grisetti2010tutorial}. An example pose graph with landmark-based landmarks is shown in Figure \ref{fig:graphslam}.

\begin{figure}
\centering
    \def\svgwidth{\textwidth}
    \import{./figs/}{graphslam.pdf_tex}
    \caption{Robot poses (triangles) and unique landmarks (stars) form a pose graph on a 2D map. Edges between robot poses indicate odometry measurements. Edges between robot poses and landmarks indicate range and bearing measurements. Upon a loop closure, here the re-discovery of landmark three, all poses inbetween the events can be adjusted. }\label{fig:graphslam}
\end{figure}

\subsection{SLAM as a Maximum-Likelihood Estimation Problem}
The classical formulation of SLAM describes the problem as maximizing the posterior probability of all points on the robot's trajectory given the odometry input and the observations. Formally,
\begin{equation}
p(x_{1:T},m|z_{1:T},u_{1:T})
\end{equation}
where $ x_{1:T}$ are all discrete positions from time 1 to time $ T$, $ z$ are the observations, and $ u$ are the odometry measurements. This formulation makes heavily use of the temporal structure of the problem. In practice, solving the SLAM problem requires
\begin{enumerate}
\item A motion update model, i.e., the probability $ p(x_t|x_{t-1},u_t)$ to be at location $ x_t$ given an odometry measurement $ u_t$ and being at location $ x_{t-1}$.
\item  A sensor model, i.e., the probability $ p(z_t|x_t,m_t)$ to make observation $ z_t$ given the robot is at location $ x_t$ and the map $ m_t$.
\end{enumerate}
A possible solution to this problem is provided by the Extended Kalman Filter, which maintains a probability density function for the robot pose as well as the positions of all landmarks on the map. Being able to uniquely identify landmarks in the environment is of outmost importance and is known as the data association problem. Like EKF-based SLAM, graph-based SLAM does not solve this problem and will fail if landmarks are confused.

In graph-based SLAM, a robot's trajectory forms the nodes of a graph whose edges are transformations (translation and rotation) that have a variance associated with it. An alternative view is the spring-mass analogy mentioned above. Instead of having each spring wiggle a node into place, graph-based SLAM aims at finding those locations that maximize the joint likelihood of all observations. As such, graph-based SLAM is a \emph{maximum likelihood estimation}\index{Maximum Likelihood Estimation} problem.

Lets revisit the normal distribution:
\begin{equation}
\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
\end{equation}

It provides the probability for a measurement to have value $ x$ given that this measurement is normal distributed with mean $ \mu$ and variance $ \sigma^2$.  We can now associate such a distribution with every node-to-node transformation, also known as a constraint. This can be pairs of distance and angle, e.g. We denote the measurement of a transformation between node $i$ and a node $j$ as $ z_{ij}$. Its expected value is denoted $ \hat{z}_{ij}$. This value is expected for example based on a map of the environment that consists of previous observations.

Formulating a normal distribution of measurements $ z_{ij}$ with mean $ \hat{z}_{ij}$ and a covariance matrix $ \Sigma_{ij}$ (containing all variances of the components of $ z_{ij}$ in its diagonal) is now straightforward. As graph-based SLAM is most often formulated as  information filter, usually the inverse of the covariance matrix (aka information matrix) is used, which we denote by $ \Omega_{ij}=\Sigma_{ij}^{-1}$.

As we are interested in maximizing the joint probability of all measurements $ \prod{z_{ij}}$ over all edge pairings $ ij$ following the maximum likelihood estimation framework, it is customary to express the PDF using the log-likelihood. By taking the natural logarithm on both sides of the PDF expression, the exponential function vanishes and $ ln \prod{z_{ij}}$ becomes $ \sum{ ln z_{ij}}$ or $ \sum{l_{ij}}$, where $ l_{ij}$ is the log-likelihood distribution for $ z_{ij}$.
\begin{equation}
l_{ij} \propto (z_{ij}-\hat{z}_{ij}(x_i,x_j))^T\Omega_{ij}(z_{ij}-\hat{z}_{ij}(x_i,x_j))
\end{equation}

Again, the log-likelihood for observation $ z_{ij}$ is directly derived from the definition of the normal distribution, but using the information matrix instead of the covariance matrix and is ridden of the exponential function by taking the logarithm on both sides.

The optimization problem can now be formulated as
\begin{equation}\label{eq:slamEM}
x^* = \arg \min_{x}\sum_{<i,j>\in \mathcal{C}}e_{ij}^T\Omega_{ij}e_{ij}
\end{equation}
with $ e_{ij}(x_i,x_j)=z_{ij}-\hat{z}_{ij}(x_i,xj)$ the error between measurement and expected value. Note that the sum actually needs to be minimized as the individual terms are technically the negative log-likelihood.

\subsection{Numerical Techniques for Graph-based SLAM}
Solving the MLE problem is non-trivial, especially if the number of constraints provided, i.e., observations that relate one landmark to another, is large. A classical approach is to linearize the problem at the current configuration and reducing it to a problem of the form $ Ax=b$. The intuition here is to calculate the impact of small changes in the positions of all nodes on all $ e_{ij}$. After performing this motion, linearization and optimization can be  repeated until convergence. 

Recently, more powerful numerical methods have been developed. Instead of solving the MLE, one can employ a stochastic gradient descent algorithm. A gradient descent algorithm is an iterative approach to find the optimum of a function by moving along its gradient. Whereas a gradient descent algorithm would calculate the gradient on a fitness landscape from all available constraints, a stochastic gradient descent picks only a (non-necessarily random) subset. Intuitive examples are fitting a line to a set of $n$ points, but taking only a subset of these points when calculating the next best guess. As gradient descent works iteratively, the hope is that the algorithm takes a large part of the constraints into account. For solving Graph-based SLAM, a stochastic gradient descent algorithm would not take into account all constraints available to the robot, but  iteratively work on one constraint after the other. Here, constraints are observations on the mutual pose of nodes $i$ and $j$. Optimizing these constraints now requires moving both nodes $i$ and $j$ so that the error between where the robot thinks the nodes should be and what it actually sees gets reduced.  As this is a trade-off between multiple, maybe conflicting observations, the result will approximate a Maximum Likelihood estimate.

More specifically, with $ e_{ij}$ the error between an observation and what the robot expects to see, based on its previous observation and sensor model, one can distribute the error along the entire trajectory between both landmarks that are involved in the constraint. That is, if the constraint involves landmarks $i$ and $j$, not only $i$ and $j$'s pose will be updated but all points in-between will be moved a tiny bit.

%This approach is cumbersome and quickly gets out of control if a robot is mapping an environment over multiple hours --- leading to millions of nodes in the graph and constraints. To overcome this problem, [Gris07] propose to (1) merge nodes of a graph as it is build up by relying on accurate localization of the robot within the existing map and (2) to chose a different graph representation.

In Graph-based SLAM, edges encode the relative translation and rotation from one node to the other. Thus, altering a relationship between two nodes will require to propagate to all nodes in the network. This is because the graph is essentially a chain of nodes whose edges consist of odometry measurements. This chain then becomes a graph whenever observations (using any sensor) introduce additional constraints. Whenever such a ``loop-closure'' occurs, the resulting error will be distributed over the entire trajectory that connects the two nodes. This is not always necessary, for example when considering the robot driving a figure-8 pattern. If a loop-closure occurs in one half of the 8, the nodes in the other half of the 8 are probably not involved.

This can be addressed by constructing a  minimum spanning-tree  (MST) of the constraint graph. The MST is constructed by doing a Depth-First Search (DFS) on the constraint graph following odometry constraints. At a loop-closure, i.e., an edge in the graph that imposes a constraint to a previously seen pose, the DFS backtracks to this node and continues from there to construct the spanning tree. Updating all poses affected by this new constraint still requires modifying all nodes along the path between the two landmarks that are involved, but inserting additional constraints is greatly simplified. Whenever a robot observes new relationships between any two nodes, only the nodes on the shortest path between  the two landmarks on the MST need to be updated. %Example graphs illustrating this are shown in Figures 2a and 2b in [Gris07].
%\subsection*{Further reading}
%\begin{itemize}
%\item E. Olson, J. Leonard and S. Teller. Fast Iterative Alignment of Pose Graphs with Poor Initial Estimates. Proc. of ICRA, pp 2262-2269, Orlando, FL, 2006.

%\item G. Grisetti, C. Stachniss, S. Grzonka and W. Burgard. A Tree Parameterization for Efficiently Computing Maximum Likelihood Maps using Gradient Descent. Robotics: Science and Systems (RSS), Atlanta, GA, USA, 2007.
%\end{itemize}


\section*{Take-home lessons}
\begin{itemize}
\item Simultaneous Localization and Mapping (SLAM) is a key capability for mobile robots to operate autonomously in the world. 
\item There exist robust implementations for environments with strong landmarks, that is landmarks that can be reliably localized and identified, that use   different forms of optimization to find a collection of poses that are most likely given the available measurements. 
\item SLAM strongly benefits from additional sensors, that can provide additional evidence, in particular beacon-based sensors such as GPS.
\item How to deal with environments with dynamical objects, that is changing maps, remains an open problem. 
\end{itemize}  

\section*{Exercises}\small
\begin{enumerate}
\item In the following, you will develop a basic EKF-based SLAM system with known landmarks:
\begin{enumerate}
\item Implement a single-landmark SLAM system. Implement
basic odometry in a simulator of your choice as well as a detector to measure the angle and distance to a single landmark. Initialize your first measurement with the mean and variance from your odometry measurement and show how additional measurements can provide a bound on the odometry error using an Kalman filter. 
\item Introduce an additional landmark and let the robot  return to the first landmark after visiting the second landmark. What can you say about the variance of the second landmark after correcting your variance when reaching the first landmark for the second time?
\item Implement a simulation environment that consists of multiple distinct landmarks. The \emph{Ratslife} world is a good example (see Figure \ref{fig:ratslife}). Experimentally determine the average variance when localizing against your landmarks. How you do this will depend on the tools already at your disposal. It is `OK' to cheat, for example by providing the robot with a list of landmarks close by and simulating a range and bearing measurement. Alternatively, download an open-source SLAM dataset such as the UTIAS Mr.CLAM dataset.
\item Use the tools that you developed above to implement EKF-based SLAM.
\end{enumerate}
\item EKF-based SLAM requires landmarks to be uniquely identifiable. Think about a possible implementation using only corner and wall detectors. How could you make these landmarks appear to be unique and what is the limitation of this approach? 
\item In the following, you will develop a basic Graph-based SLAM system:
\begin{enumerate}
\item Implement a graph data structure that allows you to maintain pose of the robot and landmarks in the environment. Store translation and rotation from node to node or to the landmark, respectively, on each edge.
\item Implement a Depth-First Search algorithm that allows you to compute the shortest path between two nodes on the graph.
\item Use your own simulator or a canned dataset to implement basic graph-based SLAM. Upon loop closure, update your pose estimate based on the landmark position by averaging between. Use your new estimate to update previous poses along the shortest path back to the previous pose at which the landmark has been observed. Experiment with different policies to update your pose and document your findings. 
\end{enumerate}
\end{enumerate}

